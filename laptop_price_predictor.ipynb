{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Laptop Price Predictor\n",
        "\n",
        "\n",
        "Short project notebook to train a simple Machine Learning model to predict laptop prices.\n",
        "\n",
        "This notebook is meant to be uploaded to your GitHub profile as a demo project. Replace `laptop_price.csv` with your real dataset or keep using the synthetic dataset provided for demonstration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Overview\n",
        "\n",
        "- Problem: Predict the price (in INR or USD) of laptops given features such as brand, ram, storage, processor, weight, and so on.\n",
        "- Approach: Simple end-to-end pipeline \u2014 load data, do exploratory data analysis (EDA), preprocess, train a `RandomForestRegressor`, evaluate, and save the model as a `.pkl` file.\n",
        "- Notes: If you have a `laptop_price.csv`, put it in the same folder. If not found, this notebook will create a small synthetic dataset so the pipeline runs end-to-end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Requirements\n",
        "\n",
        "```bash\n",
        "pip install pandas scikit-learn numpy matplotlib\n",
        "```\n",
        "\n",
        "All standard libraries are used. If you run on GitHub (rendered preview), outputs may not execute there \u2014 but the notebook file and code are visible to recruiters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import pickle\n",
        "\n",
        "print('Libraries imported')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Load dataset (or create synthetic if not present)\n",
        "DATA_PATH = 'laptop_price.csv'\n",
        "if os.path.exists(DATA_PATH):\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f'Loaded dataset from {DATA_PATH} \u2014 shape:', df.shape)\n",
        "else:\n",
        "    print(f'{DATA_PATH} not found. Creating a small synthetic dataset for demo.')\n",
        "    rng = np.random.RandomState(42)\n",
        "    brands = ['Dell', 'HP', 'Lenovo', 'Asus', 'Acer']\n",
        "    processors = ['i3', 'i5', 'i7']\n",
        "    rams = [4, 8, 16, 32]\n",
        "    storages = [256, 512, 1024]\n",
        "    weights = [1.1, 1.3, 1.5, 2.0]\n",
        "    n = 200\n",
        "    df = pd.DataFrame({\n",
        "        'brand': rng.choice(brands, n),\n",
        "        'processor': rng.choice(processors, n),\n",
        "        'ram_gb': rng.choice(rams, n),\n",
        "        'storage_gb': rng.choice(storages, n),\n",
        "        'weight_kg': rng.choice(weights, n),\n",
        "        'price': None\n",
        "    })\n",
        "    # Generate a synthetic price with some logic + noise\n",
        "    base = df['ram_gb'] * 2 + (df['storage_gb'] / 128) * 5\n",
        "    proc_map = {'i3': 20, 'i5': 40, 'i7': 60}\n",
        "    brand_map = {'Dell': 10, 'HP': 5, 'Lenovo': 8, 'Asus': 6, 'Acer': 4}\n",
        "    df['price'] = (base + df['processor'].map(proc_map) + df['brand'].map(brand_map) - df['weight_kg'] * 3 + rng.normal(0, 8, n)).round(2)\n",
        "    df.to_csv('synthetic_laptop_price_demo.csv', index=False)\n",
        "    print('Synthetic dataset created with shape:', df.shape)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Basic EDA\n",
        "\n",
        "Let's look at basic statistics and feature distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df.describe(include='all'))\n",
        "print('\\nMissing values per column:')\n",
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Preprocessing & Pipeline\n",
        "\n",
        "We'll one-hot encode categorical features and scale numeric features. Then a RandomForestRegressor will be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define features\n",
        "TARGET = 'price'\n",
        "FEATURES = [c for c in df.columns if c != TARGET]\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if TARGET in numerical_cols:\n",
        "    numerical_cols.remove(TARGET)\n",
        "\n",
        "print('Categorical cols:', categorical_cols)\n",
        "print('Numerical cols:', numerical_cols)\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_cols),\n",
        "        ('cat', cat_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
        "\n",
        "print('Pipeline ready')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train / Test split and training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df[FEATURES]\n",
        "y = df[TARGET]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Model trained')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "preds = pipeline.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, preds)\n",
        "r2 = r2_score(y_test, preds)\n",
        "print(f'Mean Absolute Error: {mae:.3f}')\n",
        "print(f'R^2 score: {r2:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Save model to a pickle file\n",
        "model_filename = 'laptop_price_model.pkl'\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(pipeline, f)\n",
        "print(f'Model saved to {model_filename}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. Example: load model and predict on a single sample\n",
        "with open(model_filename, 'rb') as f:\n",
        "    loaded = pickle.load(f)\n",
        "\n",
        "sample = X_test.iloc[0:1]\n",
        "print('Sample features:\\n', sample)\n",
        "print('Predicted price:', loaded.predict(sample))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. How to upload this project to GitHub\n",
        "\n",
        "1. Create a new repository on GitHub (e.g., `laptop-price-predictor`).\n",
        "2. Add this notebook file `laptop_price_predictor.ipynb` to the repository root.\n",
        "3. If you used a separate CSV, upload it as well (e.g., `laptop_price.csv`).\n",
        "4. Add a `README.md` describing the project, dependencies, and sample results.\n",
        "5. Commit and push. Recruiters will be able to preview the notebook on GitHub (static view) and download to run locally.\n",
        "\n",
        "Optional: Include `requirements.txt` with pinned package versions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n",
        "You can edit this notebook (author name, dataset path, model choices) before uploading. Good luck!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
